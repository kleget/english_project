# database_operations.py
from config import *
import sqlite3 as sq
import os
from pathlib import Path
from translation_utils import translate_batch

# Ensure base database directory exists before any operations
Path('database').mkdir(exist_ok=True)


def create_table(db_name: str, table_name: str):
    if not os.path.exists('database'):
        os.makedirs('database')
    if '.db' not in db_name:
        db_name += '.db'

    if table_name == 'from_all_files':
        schema = "id INTEGER PRIMARY KEY AUTOINCREMENT, deleted_word TEXT NOT NULL, count INTEGER NOT NULL, merged_to TEXT NOT NULL"
    else:
        schema = "id INTEGER PRIMARY KEY AUTOINCREMENT, word TEXT NOT NULL, count INTEGER NOT NULL"

    db_path = Path('database') / f"{db_name}.db" if '.db' not in db_name else Path('database') / db_name
    with sq.connect(db_path) as con:
        sql = con.cursor()
        sql.execute(f"CREATE TABLE IF NOT EXISTS {table_name} ({schema})")
        con.commit()


def insert_many_into_table(db_name: str, table_name: str, data: list):
    if '.db' not in db_name:
        db_name += '.db'
    create_table(db_name, table_name)

    if table_name == 'from_all_files':
        columns = ('deleted_word', 'count', 'merged_to')
        query = f"INSERT INTO {table_name} {columns} VALUES (?, ?, ?)"
    else:
        columns = ('word', 'count')
        query = f"INSERT INTO {table_name} {columns} VALUES (?, ?)"

    db_path = Path('database') / db_name
    with sq.connect(db_path) as con:
        sql = con.cursor()
        formatted_data = list({tuple(item) for item in data})
        sql.executemany(query, formatted_data)
        con.commit()


def select_from_table(db_name: str, request: str):
    if '.db' not in db_name:
        db_name += '.db'
    db_path = Path('database') / db_name
    with sq.connect(db_path) as con:
        sql = con.cursor()
        sql.execute(request)
        return [item[0] for item in sql.fetchall()]



def create_intersection_table_query(tables, db_path: Path, result_table: str = "word_intersection"): 
    """
    Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ Ð¿ÐµÑ€ÐµÑÐµÑ‡ÐµÐ½Ð¸Ñ Ð² ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ð¾Ð¹ Ð‘Ð”.
    """
    if not db_path.exists():
        print(f"Ð¤Ð°Ð¹Ð» Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ… {db_path} Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½!")
        return False

    try:
        conn = sq.connect(db_path)
        cursor = conn.cursor()

        # Ð£Ð´Ð°Ð»ÑÐµÐ¼ ÑÑ‚Ð°Ñ€ÑƒÑŽ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ
        cursor.execute(f"DROP TABLE IF EXISTS {result_table}")

        # Ð—Ð°Ð¿Ñ€Ð¾Ñ: Ð½Ð°Ð¹Ñ‚Ð¸ ÑÐ»Ð¾Ð²Ð°, Ð¿Ñ€Ð¸ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ Ð²Ð¾ Ð²ÑÐµÑ… Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ð°Ñ…
        union_words = ' UNION ALL '.join([f"SELECT word, '{table}' as src FROM {table}" for table in tables])
        intersection_words = f"""
            SELECT word FROM (
                SELECT word, COUNT(DISTINCT src) as cnt FROM ({union_words})
                GROUP BY word HAVING cnt = {len(tables)}
            )
        """

        # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ Ñ ÑÑƒÐ¼Ð¼Ð¾Ð¹ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ ÑÑ‚Ð¸Ñ… ÑÐ»Ð¾Ð²
        union_counts = ' UNION ALL '.join([f"SELECT word, count FROM {table}" for table in tables])
        sum_query = f"""
            CREATE TABLE {result_table} AS
            SELECT 
                t.word,
                SUM(t.count) as count
            FROM ({union_counts}) t
            WHERE t.word IN ({intersection_words})
            GROUP BY t.word
            ORDER BY count DESC
        """
        cursor.execute(sum_query)
        conn.commit()

        total = cursor.execute(f"SELECT COUNT(*) FROM {result_table}").fetchone()[0]
        print(f"Ð¡Ð¾Ð·Ð´Ð°Ð½Ð° Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ð° {result_table} Ñ {total} ÑÐ»Ð¾Ð²Ð°Ð¼Ð¸.")
        conn.close()
        return True

    except Exception as e:
        print(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ð¸ Ð¿ÐµÑ€ÐµÑÐµÑ‡ÐµÐ½Ð¸Ñ: {e}")
        conn.rollback()
        conn.close()
        return False



def create_translations_table(cursor, table_name: str = "translations"):
    """
    Ð¡Ð¾Ð·Ð´Ð°ÐµÑ‚ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ Ð´Ð»Ñ Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð¾Ð² ÑÐ»Ð¾Ð².
    """
    cursor.execute(f"""
        CREATE TABLE IF NOT EXISTS {table_name} (
            word TEXT PRIMARY KEY,
            count INTEGER NOT NULL,
            translation TEXT NOT NULL
        )
    """)
    cursor.execute(f"CREATE INDEX IF NOT EXISTS idx_{table_name}_word ON {table_name}(word)")


def create_union_table_query(tables, db_path: Path, result_table: str = "global_union", 
                             translation_threshold: float = 0.6):
    """
    Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÑ‚ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ñ Ð² ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ð¾Ð¹ Ð‘Ð”.
    ÐŸÐµÑ€ÐµÐ²Ð¾Ð´Ñ‹ Ñ…Ñ€Ð°Ð½ÑÑ‚ÑÑ Ð² Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ðµ translations Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ñ‡Ð°ÑÑ‚Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼Ñ‹Ñ… ÑÐ»Ð¾Ð².
    
    :param translation_threshold: Target share of total token frequency to cover with translations (0.6 = 60% of usage)
    """
    # translation_threshold Ð·Ð°Ð´Ð°Ñ‘Ñ‚ Ð´Ð¾Ð»ÑŽ Ð²ÑÐµÑ… Ð²Ñ…Ð¾Ð¶Ð´ÐµÐ½Ð¸Ð¹, ÐºÐ¾Ñ‚Ð¾Ñ€ÑƒÑŽ Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð¿Ð¾ÐºÑ€Ñ‹Ð²Ð°Ñ‚ÑŒ Ð¿ÐµÑ€ÐµÐ²ÐµÐ´Ñ‘Ð½Ð½Ñ‹Ðµ ÑÐ»Ð¾Ð²Ð°.
    if not db_path.exists():
        print(f"Ð¤Ð°Ð¹Ð» Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ… {db_path} Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½!")
        return False

    try:
        conn = sq.connect(db_path)
        cursor = conn.cursor()

        # Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ global_union (translations ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð´Ð»Ñ ÐºÑÑˆÐ°)
        cursor.execute(f"DROP TABLE IF EXISTS {result_table}")

        # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ global_union Ð‘Ð•Ð— ÐºÐ¾Ð»Ð¾Ð½ÐºÐ¸ translation
        cursor.execute(f"""
            CREATE TABLE {result_table} (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                word TEXT NOT NULL,
                count INTEGER NOT NULL
            )
        """)

        # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½ÑƒÑŽ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ Ð´Ð»Ñ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð¾Ð²
        create_translations_table(cursor)

        # Ð¡Ð¾Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð¸ ÑÑƒÐ¼Ð¼Ð¸Ñ€ÑƒÐµÐ¼ Ð²ÑÐµ ÑÐ»Ð¾Ð²Ð°
        union_counts = ' UNION ALL '.join([f"SELECT word, count FROM {table}" for table in tables])
        cursor.execute(f"""
            SELECT word, SUM(count) as count
            FROM ({union_counts})
            GROUP BY word
            ORDER BY count DESC
        """)
        word_data = cursor.fetchall()

        # Ð’ÑÑ‚Ð°Ð²Ð»ÑÐµÐ¼ Ð²ÑÐµ ÑÐ»Ð¾Ð²Ð° Ð² global_union
        insert_query = f"INSERT INTO {result_table} (word, count) VALUES (?, ?)"
        cursor.executemany(insert_query, word_data)

        # Ð’Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ ÐºÐ°Ð½Ð´Ð¸Ð´Ð°Ñ‚Ð¾Ð² Ð½Ð° Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´, Ð¾Ñ€Ð¸ÐµÐ½Ñ‚Ð¸Ñ€ÑƒÑÑÑŒ Ð½Ð° Ñ€ÐµÐ°Ð»ÑŒÐ½Ð¾ Ð¿Ð¾Ð¿ÑƒÐ»ÑÑ€Ð½Ñ‹Ðµ ÑÐ»Ð¾Ð²Ð°
        def pick_popular_words(data, coverage_ratio=0.6, min_ratio=0.05, max_ratio=0.4, min_count=5):
            if not data:
                return []

            total_unique = len(data)
            total_occurrences = sum(item[1] for item in data)
            if total_occurrences == 0:
                return data[:max(1, int(total_unique * min_ratio))]

            # min_ratio/max_ratio Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð²Ð°ÑŽÑ‚ Ð´Ð¾Ð»ÑŽ ÑÐ»Ð¾Ð², Ð° coverage_ratio Ð¾Ñ‚Ð²ÐµÑ‡Ð°ÐµÑ‚ Ð·Ð° Ð´Ð¾Ð»ÑŽ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ñ‹
            min_words = max(1, int(total_unique * min_ratio))
            max_words = max(min_words, int(total_unique * max_ratio))
            coverage_target = max(0.1, min(0.95, coverage_ratio))

            cumulative = 0
            cutoff_count = data[-1][1]
            selected = []

            for idx, row in enumerate(data):
                selected.append(row)
                cumulative += row[1]
                cutoff_count = row[1]

                # ÐžÑÑ‚Ð°Ð½Ð°Ð²Ð»Ð¸Ð²Ð°ÐµÐ¼ÑÑ, ÐºÐ¾Ð³Ð´Ð° Ð´Ð¾ÑÑ‚Ð¸Ð³Ð»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾Ð³Ð¾ Ð¿Ð¾ÐºÑ€Ñ‹Ñ‚Ð¸Ñ Ð¸Ð»Ð¸ ÑƒÐ¿Ñ‘Ñ€Ð»Ð¸ÑÑŒ Ð² Ð»Ð¸Ð¼Ð¸Ñ‚Ñ‹
                if idx + 1 >= min_words and cumulative >= total_occurrences * coverage_target:
                    break
                if idx + 1 >= max_words:
                    break

            # ÐžÑ‚ÑÐµÐ¸Ð²Ð°ÐµÐ¼ Ð²ÑÑ‘, Ñ‡Ñ‚Ð¾ Ñ€ÐµÐ¶Ðµ Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¿Ð¾Ñ€Ð¾Ð³Ð°
            effective_cutoff = max(cutoff_count, min_count)
            popular = [row for row in data if row[1] >= effective_cutoff]

            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑÐ»Ð¾Ð² Ð² Ñ€Ð°Ð¼ÐºÐ°Ñ… Ð·Ð°Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð³Ñ€Ð°Ð½Ð¸Ñ†
            if len(popular) > max_words:
                popular = popular[:max_words]
            if len(popular) < min_words:
                popular = data[:min_words]

            return popular
        
        ''' min_ratio Ð¸ max_ratio Ð·Ð°Ð´Ð°ÑŽÑ‚ Ð¼Ð¸Ð½Ð¸Ð¼Ð°Ð»ÑŒÐ½ÑƒÑŽ Ð¸ Ð¼Ð°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½ÑƒÑŽ Ð´Ð¾Ð»ÑŽ ÑÐ»Ð¾Ð², 
            ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð²Ð¾Ð¾Ð±Ñ‰Ðµ Ð¼Ð¾Ð³ÑƒÑ‚ Ð¿Ð¾Ð¿Ð°ÑÑ‚ÑŒ Ð² ÑÐ¿Ð¸ÑÐ¾Ðº Ð½Ð° Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´. 
            Ð¡ÐµÐ¹Ñ‡Ð°Ñ ÑÑ‚Ð¾ 5% Ð¸ 40% ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÐµÐ½Ð½Ð¾ â€” Ð´Ð°Ð¶Ðµ ÐµÑÐ»Ð¸ ÑÐ»Ð¾Ð² Ð¼Ð°Ð»Ð¾ Ð¸Ð»Ð¸ Ð¾Ñ‡ÐµÐ½ÑŒ Ð¼Ð½Ð¾Ð³Ð¾, 
            Ð¼Ñ‹ Ð½Ðµ Ð²Ñ‹Ð¹Ð´ÐµÐ¼ Ð·Ð° ÑÑ‚Ð¸ Ñ€Ð°Ð¼ÐºÐ¸.'''

        frequent_words_data = pick_popular_words(
            word_data,
            coverage_ratio=translation_threshold,
            min_ratio=0.05,
            max_ratio=0.4,
            min_count=5
        )
        frequent_words = [row[0] for row in frequent_words_data]

        # Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ð¼ ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ Ð´Ð»Ñ Ð±Ñ‹ÑÑ‚Ñ€Ð¾Ð³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ° count Ð¿Ð¾ ÑÐ»Ð¾Ð²Ñƒ
        word_to_count = {word: count for word, count in frequent_words_data}

        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð³Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÐºÑÑˆ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð¾Ð² (Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð²ÑÐµÐ¼Ð¸ Ð½Ð°ÑƒÑ‡Ð½Ñ‹Ð¼Ð¸ Ð‘Ð”)
        print(f"ðŸ” ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð³Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÐºÑÑˆ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð¾Ð² Ð´Ð»Ñ {len(frequent_words)} ÑÐ»Ð¾Ð²...")
        cached_translations = get_cached_translations(frequent_words)
        cached_count = len(cached_translations)
        
        # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ ÑÐ»Ð¾Ð²Ð°, ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð½ÑƒÐ¶Ð½Ð¾ Ð¿ÐµÑ€ÐµÐ²ÐµÑÑ‚Ð¸ (Ð½ÐµÑ‚ Ð² Ð³Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ð¾Ð¼ ÐºÑÑˆÐµ)
        words_to_translate = [word for word in frequent_words if word not in cached_translations]
        new_translations = {}
        
        if words_to_translate:
            print(f"ðŸ” ÐŸÐµÑ€ÐµÐ²Ð¾Ð´Ð¸Ð¼ {len(words_to_translate)} Ð½Ð¾Ð²Ñ‹Ñ… ÑÐ»Ð¾Ð² (Ð¸Ð· Ð³Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ ÐºÑÑˆÐ°: {cached_count})...")
            translations_list = translate_batch(words_to_translate)
            
            if translations_list and len(translations_list) == len(words_to_translate):
                
                new_translations = {
                    word: trans
                    for word, trans in zip(words_to_translate, translations_list)
                    if trans and trans != '""' and trans.strip() and trans.lower() != word.lower()
                }
# Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð½Ð¾Ð²Ñ‹Ðµ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ñ‹ Ð² Ð³Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÐºÑÑˆ (Ð´Ð»Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ Ð´Ñ€ÑƒÐ³Ð¸Ð¼Ð¸ Ð‘Ð”)
                save_to_global_translations_cache(new_translations)
                print(f"ðŸ’¾ Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¾ {len(new_translations)} Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð¾Ð² Ð² Ð³Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÐºÑÑˆ")
            else:
                print(f"âš ï¸ ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð¾Ð² ({len(translations_list) if translations_list else 0}) Ð½Ðµ ÑÐ¾Ð²Ð¿Ð°Ð´Ð°ÐµÑ‚ Ñ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾Ð¼ ÑÐ»Ð¾Ð² ({len(words_to_translate)})")
        else:
            print(f"âœ… Ð’ÑÐµ {cached_count} ÑÐ»Ð¾Ð² Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹ Ð² Ð³Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ð¾Ð¼ ÐºÑÑˆÐµ, Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´ Ð½Ðµ Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ!")

        # ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÑÐµÐ¼ ÐºÑÑˆ Ð¸ Ð½Ð¾Ð²Ñ‹Ðµ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ñ‹
        all_translations = {**cached_translations, **new_translations}

        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð²ÑÐµ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ñ‹ Ð² Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½ÑƒÑŽ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ translations (Ð´Ð»Ñ ÑƒÐ´Ð¾Ð±ÑÑ‚Ð²Ð° Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ ÑÑ‚Ð¾Ð¹ Ð‘Ð”)
        # ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÐ¼ count Ð´Ð»Ñ ÑÐ»Ð¾Ð² Ð¸Ð· ÐºÑÑˆÐ°, Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð½Ð¾Ð²Ñ‹Ðµ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ñ‹
        if all_translations:
            # Ð£Ð´Ð°Ð»ÑÐµÐ¼ ÑÑ‚Ð°Ñ€Ñ‹Ðµ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ð´Ð»Ñ ÑÑ‚Ð¸Ñ… ÑÐ»Ð¾Ð² (ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ)
            words_list = list(all_translations.keys())
            placeholders = ','.join(['?'] * len(words_list))
            cursor.execute(f"DELETE FROM translations WHERE word IN ({placeholders})", words_list)
            
            # Ð’ÑÑ‚Ð°Ð²Ð»ÑÐµÐ¼ Ð²ÑÐµ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ñ‹ Ñ Ð°ÐºÑ‚ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ð¼ count
            translation_insert = "INSERT INTO translations (word, count, translation) VALUES (?, ?, ?)"
            translation_data = [
                (word, word_to_count[word], trans)
                for word, trans in all_translations.items()
                if word in word_to_count and trans and trans.lower() != word.lower()
            ]
            if translation_data:
                cursor.executemany(translation_insert, translation_data)
                print(f"âœ… Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¾ {len(translation_data)} Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð¾Ð² Ð² Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½ÑƒÑŽ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ translations")
        
        total_translated = len(all_translations)
        print(f"ðŸ“Š Ð˜Ñ‚Ð¾Ð³Ð¾: {total_translated} Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð¾Ð² ({cached_count} Ð¸Ð· Ð³Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ ÐºÑÑˆÐ°, {len(new_translations)} Ð½Ð¾Ð²Ñ‹Ñ…)")

        conn.commit()

        total = cursor.execute(f"SELECT COUNT(*) FROM {result_table}").fetchone()[0]
        print(f"âœ… Ð¢Ð°Ð±Ð»Ð¸Ñ†Ð° '{result_table}' ÑÐ¾Ð·Ð´Ð°Ð½Ð° Ñ {total} ÑÐ»Ð¾Ð²Ð°Ð¼Ð¸")
        conn.close()
        return True

    except Exception as e:
        print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ð¸ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð¸Ñ: {e}")
        conn.rollback()
        conn.close()
        return False


def get_global_translations_cache_path() -> Path:
    """
    Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ð¿ÑƒÑ‚ÑŒ Ðº Ñ†ÐµÐ½Ñ‚Ñ€Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¹ Ð‘Ð” ÐºÑÑˆÐ° Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð¾Ð².
    Ð­Ñ‚Ð° Ð‘Ð” Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð²ÑÐµÐ¼Ð¸ Ð½Ð°ÑƒÑ‡Ð½Ñ‹Ð¼Ð¸ Ð‘Ð” Ð´Ð»Ñ Ð¿ÐµÑ€ÐµÐ¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð¾Ð².
    
    ÐšÑÑˆ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð² ÐžÐ‘Ð Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ (enâ†’ru Ð¸ ruâ†’en), Ñ‡Ñ‚Ð¾ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚
    Ð¿ÐµÑ€ÐµÐ¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ñ‹ Ð½ÐµÐ·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ Ð¾Ñ‚ ÑÐ·Ñ‹ÐºÐ° Ð¸ÑÑ…Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ñ‚ÐµÐºÑÑ‚Ð°.
    """
    cache_db_path = Path('database') / 'translations_cache.db'
    return cache_db_path


def init_global_translations_cache():
    """
    Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Ñ†ÐµÐ½Ñ‚Ñ€Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½ÑƒÑŽ Ð‘Ð” ÐºÑÑˆÐ° Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð¾Ð².
    Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ñ‚ Ð‘Ð” Ð¸ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñƒ, ÐµÑÐ»Ð¸ Ð¸Ñ… Ð½ÐµÑ‚.
    
    ÐšÑÑˆ Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ñ‹ Ð² ÐžÐ‘Ð Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ (enâ†’ru Ð¸ ruâ†’en),
    Ñ‡Ñ‚Ð¾ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð¿ÐµÑ€ÐµÐ¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ñ‹ Ð½ÐµÐ·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ Ð¾Ñ‚ Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ.
    """
    cache_path = get_global_translations_cache_path()
    cache_path.parent.mkdir(parents=True, exist_ok=True)
    
    conn = sq.connect(cache_path)
    cursor = conn.cursor()
    
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS translations_cache (
            word TEXT PRIMARY KEY,
            translation TEXT NOT NULL
        )
    """)
    cursor.execute("CREATE INDEX IF NOT EXISTS idx_translations_cache_word ON translations_cache(word)")
    
    conn.commit()
    conn.close()


def get_cached_translations(words: list, use_global_cache: bool = True) -> dict:
    """
    ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÑ‚ ÐºÑÑˆÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ñ‹ Ð¸Ð· Ñ†ÐµÐ½Ñ‚Ñ€Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ð¾Ð³Ð¾ ÐºÑÑˆÐ°.
    Ð Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð´Ð»Ñ Ð¾Ð±Ð¾Ð¸Ñ… Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ð¹ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð° (enâ†’ru Ð¸ ruâ†’en),
    Ñ‚Ð°Ðº ÐºÐ°Ðº ÐºÑÑˆ Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ñ‹ Ð² Ð¾Ð±Ðµ ÑÑ‚Ð¾Ñ€Ð¾Ð½Ñ‹.
    
    :param words: Ð¡Ð¿Ð¸ÑÐ¾Ðº ÑÐ»Ð¾Ð² Ð´Ð»Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ¸ (Ð¼Ð¾Ð³ÑƒÑ‚ Ð±Ñ‹Ñ‚ÑŒ ÐºÐ°Ðº Ð°Ð½Ð³Ð»Ð¸Ð¹ÑÐºÐ¸Ðµ, Ñ‚Ð°Ðº Ð¸ Ñ€ÑƒÑÑÐºÐ¸Ðµ)
    :param use_global_cache: Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð»Ð¸ Ð³Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÐºÑÑˆ (Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ True)
    :return: Ð¡Ð»Ð¾Ð²Ð°Ñ€ÑŒ {word: translation} Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð½Ð°Ð¹Ð´ÐµÐ½Ð½Ñ‹Ñ… ÑÐ»Ð¾Ð²
    """
    if not words:
        return {}
    
    if not use_global_cache:
        return {}
    
    # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ ÐºÑÑˆ, ÐµÑÐ»Ð¸ ÐµÐ³Ð¾ Ð½ÐµÑ‚
    init_global_translations_cache()
    
    cache_path = get_global_translations_cache_path()
    conn = sq.connect(cache_path)
    cursor = conn.cursor()
    
    try:
        placeholders = ','.join(['?'] * len(words))
        cursor.execute(f"""
            SELECT word, translation 
            FROM translations_cache 
            WHERE word IN ({placeholders})
        """, words)
        
        rows = cursor.fetchall()
        # ÐžÑ‚Ð±Ñ€Ð°ÑÑ‹Ð²Ð°ÐµÐ¼ Ð¿ÑƒÑÑ‚Ñ‹Ðµ Ð¸ Ñ‚Ð¾Ð¶Ð´ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ðµ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ñ‹ (word == translation), Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð´Ð°Ñ‚ÑŒ ÑˆÐ°Ð½Ñ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð½Ð¾Ñ€Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´
        result = {w: t for w, t in rows if t and t.strip() and t.lower() != w.lower()}
    finally:
        conn.close()

    return result


def save_to_global_translations_cache(translations: dict):
    """
    Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ñ‹ Ð² Ñ†ÐµÐ½Ñ‚Ñ€Ð°Ð»Ð¸Ð·Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ ÐºÑÑˆ Ð² ÐžÐ‘Ð Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ.
    Ð­Ñ‚Ð¾ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ ÐºÑÑˆ ÐºÐ°Ðº Ð´Ð»Ñ enâ†’ru, Ñ‚Ð°Ðº Ð¸ Ð´Ð»Ñ ruâ†’en Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ð¾Ð².
    
    :param translations: Ð¡Ð»Ð¾Ð²Ð°Ñ€ÑŒ {word: translation}
    """
    if not translations:
        return
    
    # Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼ ÐºÑÑˆ, ÐµÑÐ»Ð¸ ÐµÐ³Ð¾ Ð½ÐµÑ‚
    init_global_translations_cache()
    
    cache_path = get_global_translations_cache_path()
    conn = sq.connect(cache_path)
    cursor = conn.cursor()
    
    try:
        # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ INSERT OR IGNORE, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ð¿ÐµÑ€ÐµÐ·Ð°Ð¿Ð¸ÑÑ‹Ð²Ð°Ñ‚ÑŒ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ñ‹
        insert_query = "INSERT OR IGNORE INTO translations_cache (word, translation) VALUES (?, ?)"
        
        # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ñ‹ Ð² ÐžÐ‘Ð Ð½Ð°Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ñ:
        # 1. word â†’ translation (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€: "hello" â†’ "Ð¿Ñ€Ð¸Ð²ÐµÑ‚")
        # 2. translation â†’ word (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€: "Ð¿Ñ€Ð¸Ð²ÐµÑ‚" â†’ "hello")
        translation_data = []
        for word, trans in translations.items():
            if not trans or trans == '""' or trans.lower() == word.lower():
                continue
            translation_data.append((word, trans))
            if trans.lower() != word.lower():
                translation_data.append((trans, word))

        if translation_data:
            cursor.executemany(insert_query, translation_data)
            conn.commit()
    finally:
        conn.close()


def get_word_with_translation(cursor, word: str, union_table: str = "global_union"):
    """
    Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ ÑÐ»Ð¾Ð²Ð¾ Ð¸ Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´ Ð¸Ð· union_table + translations (ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ).
    """
    cursor.execute(f"""
        SELECT u.word, u.count, t.translation
        FROM {union_table} u
        LEFT JOIN translations t ON u.word = t.word
        WHERE u.word = ?
    """, (word,))
    return cursor.fetchone()


# database_operations.py â€” ÑÐ»ÑƒÐ¶ÐµÐ±Ð½Ñ‹Ðµ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹
def create_processed_books_table(db_name: str):
    db_path = Path('database') / f"{db_name}.db"
    with sq.connect(db_path) as con:
        cursor = con.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS processed_books (
                book_path TEXT PRIMARY KEY,
                processed_at DATETIME DEFAULT CURRENT_TIMESTAMP,
                word_count INTEGER,
                hash TEXT
            )
        """)
        con.commit()


def is_book_processed(db_name: str, book_path: str) -> bool:
    """Return True if the book_path is already marked as processed."""
    db_path = Path('database') / f"{db_name}.db"
    try:
        create_processed_books_table(db_name)  # ensure processed_books exists
        with sq.connect(db_path) as con:
            cursor = con.cursor()
            cursor.execute("SELECT 1 FROM processed_books WHERE book_path = ?", (book_path,))
            return cursor.fetchone() is not None
    except Exception:
        return False


def mark_book_as_processed(db_name: str, book_path: str, word_count: int):
    """Record completion of a book with its word_count."""
    create_processed_books_table(db_name)  # ensure processed_books exists
    db_path = Path('database') / f"{db_name}.db"
    with sq.connect(db_path) as con:
        cursor = con.cursor()
        cursor.execute("""
            INSERT OR REPLACE INTO processed_books (book_path, word_count)
            VALUES (?, ?)
        """, (book_path, word_count))
        con.commit()
